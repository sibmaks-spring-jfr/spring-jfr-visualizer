// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.6.1
//   protoc               v5.29.3
// source: kafka.consumer.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";

export const protobufPackage = "io.github.sibmaks.spring.jfr.dto.protobuf.kafka.consumer";

export enum KafkaConsumerPartitionEventType {
  REVOKED = 0,
  ASSIGNED = 1,
  LOST = 2,
  UNRECOGNIZED = -1,
}

export function kafkaConsumerPartitionEventTypeFromJSON(object: any): KafkaConsumerPartitionEventType {
  switch (object) {
    case 0:
    case "REVOKED":
      return KafkaConsumerPartitionEventType.REVOKED;
    case 1:
    case "ASSIGNED":
      return KafkaConsumerPartitionEventType.ASSIGNED;
    case 2:
    case "LOST":
      return KafkaConsumerPartitionEventType.LOST;
    case -1:
    case "UNRECOGNIZED":
    default:
      return KafkaConsumerPartitionEventType.UNRECOGNIZED;
  }
}

export function kafkaConsumerPartitionEventTypeToJSON(object: KafkaConsumerPartitionEventType): string {
  switch (object) {
    case KafkaConsumerPartitionEventType.REVOKED:
      return "REVOKED";
    case KafkaConsumerPartitionEventType.ASSIGNED:
      return "ASSIGNED";
    case KafkaConsumerPartitionEventType.LOST:
      return "LOST";
    case KafkaConsumerPartitionEventType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export interface PartitionOffsets {
  currentOffset: number;
  lastCommit: number;
}

export interface KafkaConsumerStats {
  commits: number;
  commited: number;
  commitFailed: number;
  lastCommitAt: number;
}

export interface KafkaConsumerPartitionEvent {
  eventType: KafkaConsumerPartitionEventType;
  partitions: number[];
  at: number;
}

export interface KafkaConsumer {
  consumerFactory: number;
  consumerId: number;
  bootstrapServers: number;
  consumerGroup: number;
  topics: number[];
  partitions: { [key: number]: PartitionOffsets };
  stats: KafkaConsumerStats | undefined;
  partitionsEvents: KafkaConsumerPartitionEvent[];
}

export interface KafkaConsumer_PartitionsEntry {
  key: number;
  value: PartitionOffsets | undefined;
}

export interface KafkaConsumersMap {
  consumers: { [key: number]: KafkaConsumer };
}

export interface KafkaConsumersMap_ConsumersEntry {
  key: number;
  value: KafkaConsumer | undefined;
}

export interface KafkaConsumersReport {
  contexts: { [key: number]: KafkaConsumersMap };
}

export interface KafkaConsumersReport_ContextsEntry {
  key: number;
  value: KafkaConsumersMap | undefined;
}

function createBasePartitionOffsets(): PartitionOffsets {
  return { currentOffset: 0, lastCommit: 0 };
}

export const PartitionOffsets: MessageFns<PartitionOffsets> = {
  encode(message: PartitionOffsets, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.currentOffset !== 0) {
      writer.uint32(8).int64(message.currentOffset);
    }
    if (message.lastCommit !== 0) {
      writer.uint32(16).int64(message.lastCommit);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PartitionOffsets {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePartitionOffsets();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.currentOffset = longToNumber(reader.int64());
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.lastCommit = longToNumber(reader.int64());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PartitionOffsets {
    return {
      currentOffset: isSet(object.currentOffset) ? globalThis.Number(object.currentOffset) : 0,
      lastCommit: isSet(object.lastCommit) ? globalThis.Number(object.lastCommit) : 0,
    };
  },

  toJSON(message: PartitionOffsets): unknown {
    const obj: any = {};
    if (message.currentOffset !== 0) {
      obj.currentOffset = Math.round(message.currentOffset);
    }
    if (message.lastCommit !== 0) {
      obj.lastCommit = Math.round(message.lastCommit);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<PartitionOffsets>, I>>(base?: I): PartitionOffsets {
    return PartitionOffsets.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<PartitionOffsets>, I>>(object: I): PartitionOffsets {
    const message = createBasePartitionOffsets();
    message.currentOffset = object.currentOffset ?? 0;
    message.lastCommit = object.lastCommit ?? 0;
    return message;
  },
};

function createBaseKafkaConsumerStats(): KafkaConsumerStats {
  return { commits: 0, commited: 0, commitFailed: 0, lastCommitAt: 0 };
}

export const KafkaConsumerStats: MessageFns<KafkaConsumerStats> = {
  encode(message: KafkaConsumerStats, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.commits !== 0) {
      writer.uint32(8).int32(message.commits);
    }
    if (message.commited !== 0) {
      writer.uint32(16).int32(message.commited);
    }
    if (message.commitFailed !== 0) {
      writer.uint32(24).int32(message.commitFailed);
    }
    if (message.lastCommitAt !== 0) {
      writer.uint32(32).int64(message.lastCommitAt);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KafkaConsumerStats {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKafkaConsumerStats();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.commits = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.commited = reader.int32();
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.commitFailed = reader.int32();
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.lastCommitAt = longToNumber(reader.int64());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KafkaConsumerStats {
    return {
      commits: isSet(object.commits) ? globalThis.Number(object.commits) : 0,
      commited: isSet(object.commited) ? globalThis.Number(object.commited) : 0,
      commitFailed: isSet(object.commitFailed) ? globalThis.Number(object.commitFailed) : 0,
      lastCommitAt: isSet(object.lastCommitAt) ? globalThis.Number(object.lastCommitAt) : 0,
    };
  },

  toJSON(message: KafkaConsumerStats): unknown {
    const obj: any = {};
    if (message.commits !== 0) {
      obj.commits = Math.round(message.commits);
    }
    if (message.commited !== 0) {
      obj.commited = Math.round(message.commited);
    }
    if (message.commitFailed !== 0) {
      obj.commitFailed = Math.round(message.commitFailed);
    }
    if (message.lastCommitAt !== 0) {
      obj.lastCommitAt = Math.round(message.lastCommitAt);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<KafkaConsumerStats>, I>>(base?: I): KafkaConsumerStats {
    return KafkaConsumerStats.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<KafkaConsumerStats>, I>>(object: I): KafkaConsumerStats {
    const message = createBaseKafkaConsumerStats();
    message.commits = object.commits ?? 0;
    message.commited = object.commited ?? 0;
    message.commitFailed = object.commitFailed ?? 0;
    message.lastCommitAt = object.lastCommitAt ?? 0;
    return message;
  },
};

function createBaseKafkaConsumerPartitionEvent(): KafkaConsumerPartitionEvent {
  return { eventType: 0, partitions: [], at: 0 };
}

export const KafkaConsumerPartitionEvent: MessageFns<KafkaConsumerPartitionEvent> = {
  encode(message: KafkaConsumerPartitionEvent, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.eventType !== 0) {
      writer.uint32(8).int32(message.eventType);
    }
    writer.uint32(18).fork();
    for (const v of message.partitions) {
      writer.int32(v);
    }
    writer.join();
    if (message.at !== 0) {
      writer.uint32(24).int64(message.at);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KafkaConsumerPartitionEvent {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKafkaConsumerPartitionEvent();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.eventType = reader.int32() as any;
          continue;
        }
        case 2: {
          if (tag === 16) {
            message.partitions.push(reader.int32());

            continue;
          }

          if (tag === 18) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.partitions.push(reader.int32());
            }

            continue;
          }

          break;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.at = longToNumber(reader.int64());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KafkaConsumerPartitionEvent {
    return {
      eventType: isSet(object.eventType) ? kafkaConsumerPartitionEventTypeFromJSON(object.eventType) : 0,
      partitions: globalThis.Array.isArray(object?.partitions)
        ? object.partitions.map((e: any) => globalThis.Number(e))
        : [],
      at: isSet(object.at) ? globalThis.Number(object.at) : 0,
    };
  },

  toJSON(message: KafkaConsumerPartitionEvent): unknown {
    const obj: any = {};
    if (message.eventType !== 0) {
      obj.eventType = kafkaConsumerPartitionEventTypeToJSON(message.eventType);
    }
    if (message.partitions?.length) {
      obj.partitions = message.partitions.map((e) => Math.round(e));
    }
    if (message.at !== 0) {
      obj.at = Math.round(message.at);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<KafkaConsumerPartitionEvent>, I>>(base?: I): KafkaConsumerPartitionEvent {
    return KafkaConsumerPartitionEvent.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<KafkaConsumerPartitionEvent>, I>>(object: I): KafkaConsumerPartitionEvent {
    const message = createBaseKafkaConsumerPartitionEvent();
    message.eventType = object.eventType ?? 0;
    message.partitions = object.partitions?.map((e) => e) || [];
    message.at = object.at ?? 0;
    return message;
  },
};

function createBaseKafkaConsumer(): KafkaConsumer {
  return {
    consumerFactory: 0,
    consumerId: 0,
    bootstrapServers: 0,
    consumerGroup: 0,
    topics: [],
    partitions: {},
    stats: undefined,
    partitionsEvents: [],
  };
}

export const KafkaConsumer: MessageFns<KafkaConsumer> = {
  encode(message: KafkaConsumer, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.consumerFactory !== 0) {
      writer.uint32(8).int32(message.consumerFactory);
    }
    if (message.consumerId !== 0) {
      writer.uint32(16).int32(message.consumerId);
    }
    if (message.bootstrapServers !== 0) {
      writer.uint32(24).int32(message.bootstrapServers);
    }
    if (message.consumerGroup !== 0) {
      writer.uint32(32).int32(message.consumerGroup);
    }
    writer.uint32(42).fork();
    for (const v of message.topics) {
      writer.int32(v);
    }
    writer.join();
    Object.entries(message.partitions).forEach(([key, value]) => {
      KafkaConsumer_PartitionsEntry.encode({ key: key as any, value }, writer.uint32(50).fork()).join();
    });
    if (message.stats !== undefined) {
      KafkaConsumerStats.encode(message.stats, writer.uint32(58).fork()).join();
    }
    for (const v of message.partitionsEvents) {
      KafkaConsumerPartitionEvent.encode(v!, writer.uint32(66).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KafkaConsumer {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKafkaConsumer();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.consumerFactory = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.consumerId = reader.int32();
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.bootstrapServers = reader.int32();
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.consumerGroup = reader.int32();
          continue;
        }
        case 5: {
          if (tag === 40) {
            message.topics.push(reader.int32());

            continue;
          }

          if (tag === 42) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.topics.push(reader.int32());
            }

            continue;
          }

          break;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          const entry6 = KafkaConsumer_PartitionsEntry.decode(reader, reader.uint32());
          if (entry6.value !== undefined) {
            message.partitions[entry6.key] = entry6.value;
          }
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.stats = KafkaConsumerStats.decode(reader, reader.uint32());
          continue;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.partitionsEvents.push(KafkaConsumerPartitionEvent.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KafkaConsumer {
    return {
      consumerFactory: isSet(object.consumerFactory) ? globalThis.Number(object.consumerFactory) : 0,
      consumerId: isSet(object.consumerId) ? globalThis.Number(object.consumerId) : 0,
      bootstrapServers: isSet(object.bootstrapServers) ? globalThis.Number(object.bootstrapServers) : 0,
      consumerGroup: isSet(object.consumerGroup) ? globalThis.Number(object.consumerGroup) : 0,
      topics: globalThis.Array.isArray(object?.topics) ? object.topics.map((e: any) => globalThis.Number(e)) : [],
      partitions: isObject(object.partitions)
        ? Object.entries(object.partitions).reduce<{ [key: number]: PartitionOffsets }>((acc, [key, value]) => {
          acc[globalThis.Number(key)] = PartitionOffsets.fromJSON(value);
          return acc;
        }, {})
        : {},
      stats: isSet(object.stats) ? KafkaConsumerStats.fromJSON(object.stats) : undefined,
      partitionsEvents: globalThis.Array.isArray(object?.partitionsEvents)
        ? object.partitionsEvents.map((e: any) => KafkaConsumerPartitionEvent.fromJSON(e))
        : [],
    };
  },

  toJSON(message: KafkaConsumer): unknown {
    const obj: any = {};
    if (message.consumerFactory !== 0) {
      obj.consumerFactory = Math.round(message.consumerFactory);
    }
    if (message.consumerId !== 0) {
      obj.consumerId = Math.round(message.consumerId);
    }
    if (message.bootstrapServers !== 0) {
      obj.bootstrapServers = Math.round(message.bootstrapServers);
    }
    if (message.consumerGroup !== 0) {
      obj.consumerGroup = Math.round(message.consumerGroup);
    }
    if (message.topics?.length) {
      obj.topics = message.topics.map((e) => Math.round(e));
    }
    if (message.partitions) {
      const entries = Object.entries(message.partitions);
      if (entries.length > 0) {
        obj.partitions = {};
        entries.forEach(([k, v]) => {
          obj.partitions[k] = PartitionOffsets.toJSON(v);
        });
      }
    }
    if (message.stats !== undefined) {
      obj.stats = KafkaConsumerStats.toJSON(message.stats);
    }
    if (message.partitionsEvents?.length) {
      obj.partitionsEvents = message.partitionsEvents.map((e) => KafkaConsumerPartitionEvent.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<KafkaConsumer>, I>>(base?: I): KafkaConsumer {
    return KafkaConsumer.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<KafkaConsumer>, I>>(object: I): KafkaConsumer {
    const message = createBaseKafkaConsumer();
    message.consumerFactory = object.consumerFactory ?? 0;
    message.consumerId = object.consumerId ?? 0;
    message.bootstrapServers = object.bootstrapServers ?? 0;
    message.consumerGroup = object.consumerGroup ?? 0;
    message.topics = object.topics?.map((e) => e) || [];
    message.partitions = Object.entries(object.partitions ?? {}).reduce<{ [key: number]: PartitionOffsets }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[globalThis.Number(key)] = PartitionOffsets.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    message.stats = (object.stats !== undefined && object.stats !== null)
      ? KafkaConsumerStats.fromPartial(object.stats)
      : undefined;
    message.partitionsEvents = object.partitionsEvents?.map((e) => KafkaConsumerPartitionEvent.fromPartial(e)) || [];
    return message;
  },
};

function createBaseKafkaConsumer_PartitionsEntry(): KafkaConsumer_PartitionsEntry {
  return { key: 0, value: undefined };
}

export const KafkaConsumer_PartitionsEntry: MessageFns<KafkaConsumer_PartitionsEntry> = {
  encode(message: KafkaConsumer_PartitionsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== 0) {
      writer.uint32(8).int32(message.key);
    }
    if (message.value !== undefined) {
      PartitionOffsets.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KafkaConsumer_PartitionsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKafkaConsumer_PartitionsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.key = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = PartitionOffsets.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KafkaConsumer_PartitionsEntry {
    return {
      key: isSet(object.key) ? globalThis.Number(object.key) : 0,
      value: isSet(object.value) ? PartitionOffsets.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: KafkaConsumer_PartitionsEntry): unknown {
    const obj: any = {};
    if (message.key !== 0) {
      obj.key = Math.round(message.key);
    }
    if (message.value !== undefined) {
      obj.value = PartitionOffsets.toJSON(message.value);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<KafkaConsumer_PartitionsEntry>, I>>(base?: I): KafkaConsumer_PartitionsEntry {
    return KafkaConsumer_PartitionsEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<KafkaConsumer_PartitionsEntry>, I>>(
    object: I,
  ): KafkaConsumer_PartitionsEntry {
    const message = createBaseKafkaConsumer_PartitionsEntry();
    message.key = object.key ?? 0;
    message.value = (object.value !== undefined && object.value !== null)
      ? PartitionOffsets.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseKafkaConsumersMap(): KafkaConsumersMap {
  return { consumers: {} };
}

export const KafkaConsumersMap: MessageFns<KafkaConsumersMap> = {
  encode(message: KafkaConsumersMap, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.consumers).forEach(([key, value]) => {
      KafkaConsumersMap_ConsumersEntry.encode({ key: key as any, value }, writer.uint32(10).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KafkaConsumersMap {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKafkaConsumersMap();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          const entry1 = KafkaConsumersMap_ConsumersEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.consumers[entry1.key] = entry1.value;
          }
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KafkaConsumersMap {
    return {
      consumers: isObject(object.consumers)
        ? Object.entries(object.consumers).reduce<{ [key: number]: KafkaConsumer }>((acc, [key, value]) => {
          acc[globalThis.Number(key)] = KafkaConsumer.fromJSON(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: KafkaConsumersMap): unknown {
    const obj: any = {};
    if (message.consumers) {
      const entries = Object.entries(message.consumers);
      if (entries.length > 0) {
        obj.consumers = {};
        entries.forEach(([k, v]) => {
          obj.consumers[k] = KafkaConsumer.toJSON(v);
        });
      }
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<KafkaConsumersMap>, I>>(base?: I): KafkaConsumersMap {
    return KafkaConsumersMap.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<KafkaConsumersMap>, I>>(object: I): KafkaConsumersMap {
    const message = createBaseKafkaConsumersMap();
    message.consumers = Object.entries(object.consumers ?? {}).reduce<{ [key: number]: KafkaConsumer }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[globalThis.Number(key)] = KafkaConsumer.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseKafkaConsumersMap_ConsumersEntry(): KafkaConsumersMap_ConsumersEntry {
  return { key: 0, value: undefined };
}

export const KafkaConsumersMap_ConsumersEntry: MessageFns<KafkaConsumersMap_ConsumersEntry> = {
  encode(message: KafkaConsumersMap_ConsumersEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== 0) {
      writer.uint32(8).int32(message.key);
    }
    if (message.value !== undefined) {
      KafkaConsumer.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KafkaConsumersMap_ConsumersEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKafkaConsumersMap_ConsumersEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.key = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = KafkaConsumer.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KafkaConsumersMap_ConsumersEntry {
    return {
      key: isSet(object.key) ? globalThis.Number(object.key) : 0,
      value: isSet(object.value) ? KafkaConsumer.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: KafkaConsumersMap_ConsumersEntry): unknown {
    const obj: any = {};
    if (message.key !== 0) {
      obj.key = Math.round(message.key);
    }
    if (message.value !== undefined) {
      obj.value = KafkaConsumer.toJSON(message.value);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<KafkaConsumersMap_ConsumersEntry>, I>>(
    base?: I,
  ): KafkaConsumersMap_ConsumersEntry {
    return KafkaConsumersMap_ConsumersEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<KafkaConsumersMap_ConsumersEntry>, I>>(
    object: I,
  ): KafkaConsumersMap_ConsumersEntry {
    const message = createBaseKafkaConsumersMap_ConsumersEntry();
    message.key = object.key ?? 0;
    message.value = (object.value !== undefined && object.value !== null)
      ? KafkaConsumer.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseKafkaConsumersReport(): KafkaConsumersReport {
  return { contexts: {} };
}

export const KafkaConsumersReport: MessageFns<KafkaConsumersReport> = {
  encode(message: KafkaConsumersReport, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.contexts).forEach(([key, value]) => {
      KafkaConsumersReport_ContextsEntry.encode({ key: key as any, value }, writer.uint32(10).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KafkaConsumersReport {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKafkaConsumersReport();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          const entry1 = KafkaConsumersReport_ContextsEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.contexts[entry1.key] = entry1.value;
          }
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KafkaConsumersReport {
    return {
      contexts: isObject(object.contexts)
        ? Object.entries(object.contexts).reduce<{ [key: number]: KafkaConsumersMap }>((acc, [key, value]) => {
          acc[globalThis.Number(key)] = KafkaConsumersMap.fromJSON(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: KafkaConsumersReport): unknown {
    const obj: any = {};
    if (message.contexts) {
      const entries = Object.entries(message.contexts);
      if (entries.length > 0) {
        obj.contexts = {};
        entries.forEach(([k, v]) => {
          obj.contexts[k] = KafkaConsumersMap.toJSON(v);
        });
      }
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<KafkaConsumersReport>, I>>(base?: I): KafkaConsumersReport {
    return KafkaConsumersReport.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<KafkaConsumersReport>, I>>(object: I): KafkaConsumersReport {
    const message = createBaseKafkaConsumersReport();
    message.contexts = Object.entries(object.contexts ?? {}).reduce<{ [key: number]: KafkaConsumersMap }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[globalThis.Number(key)] = KafkaConsumersMap.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseKafkaConsumersReport_ContextsEntry(): KafkaConsumersReport_ContextsEntry {
  return { key: 0, value: undefined };
}

export const KafkaConsumersReport_ContextsEntry: MessageFns<KafkaConsumersReport_ContextsEntry> = {
  encode(message: KafkaConsumersReport_ContextsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== 0) {
      writer.uint32(8).int32(message.key);
    }
    if (message.value !== undefined) {
      KafkaConsumersMap.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KafkaConsumersReport_ContextsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKafkaConsumersReport_ContextsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.key = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = KafkaConsumersMap.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KafkaConsumersReport_ContextsEntry {
    return {
      key: isSet(object.key) ? globalThis.Number(object.key) : 0,
      value: isSet(object.value) ? KafkaConsumersMap.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: KafkaConsumersReport_ContextsEntry): unknown {
    const obj: any = {};
    if (message.key !== 0) {
      obj.key = Math.round(message.key);
    }
    if (message.value !== undefined) {
      obj.value = KafkaConsumersMap.toJSON(message.value);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<KafkaConsumersReport_ContextsEntry>, I>>(
    base?: I,
  ): KafkaConsumersReport_ContextsEntry {
    return KafkaConsumersReport_ContextsEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<KafkaConsumersReport_ContextsEntry>, I>>(
    object: I,
  ): KafkaConsumersReport_ContextsEntry {
    const message = createBaseKafkaConsumersReport_ContextsEntry();
    message.key = object.key ?? 0;
    message.value = (object.value !== undefined && object.value !== null)
      ? KafkaConsumersMap.fromPartial(object.value)
      : undefined;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

type KeysOfUnion<T> = T extends T ? keyof T : never;
export type Exact<P, I extends P> = P extends Builtin ? P
  : P & { [K in keyof P]: Exact<P[K], I[K]> } & { [K in Exclude<keyof I, KeysOfUnion<P>>]: never };

function longToNumber(int64: { toString(): string }): number {
  const num = globalThis.Number(int64.toString());
  if (num > globalThis.Number.MAX_SAFE_INTEGER) {
    throw new globalThis.Error("Value is larger than Number.MAX_SAFE_INTEGER");
  }
  if (num < globalThis.Number.MIN_SAFE_INTEGER) {
    throw new globalThis.Error("Value is smaller than Number.MIN_SAFE_INTEGER");
  }
  return num;
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create<I extends Exact<DeepPartial<T>, I>>(base?: I): T;
  fromPartial<I extends Exact<DeepPartial<T>, I>>(object: I): T;
}
